meta_data:
  script_path: run_scripts/mbpo_exp_script.py
  exp_name: waker2d_base
  description: Train an agent using Model-Based Policy Optimization
  num_workers: 1
  using_gpus: true
# -----------------------------------------------------------------------------
variables:
  seed: [42]

# -----------------------------------------------------------------------------
constants:

  mbpo_params:
    num_epochs: 301
    num_steps_per_epoch: 1000
    num_steps_between_train_calls: 1
    num_train_steps_per_train_call: 20
    num_steps_per_eval: 5000
    max_path_length: 1000
    min_steps_before_training: 5000

    eval_deterministic: true

    batch_size: 256
    replay_buffer_size: 1000000
    model_replay_buffer_size: 1000000
    no_terminal: false
    wrap_absorbing: false

    target_update_interval: 1
    model_train_freq: 250
    model_retrain_epochs: 1
    rollout_batch_size: 100000
    real_ratio: 0.05
    rollout_schedule: [20, 150, 1, 1]

    save_best: false
    freq_saving: 10
    save_replay_buffer: false

  bnn_params:
    net_size: 200
    num_hidden_layers: 4
    num_nets: 7
    lr: 0.001
    num_elites: 5
    batch_size: 256
    max_epochs_since_update: 5
    holdout_ratio: 0.2
    max_holdout: 5000
    batch_size: 256
    log_freq: 50

  sac_params:
    net_size: 256
    num_hidden_layers: 2
    alpha: 0.2
    reward_scale: 1.0
    discount: 0.99
    soft_target_tau: 0.005
    policy_lr: 0.0003
    qf_lr: 0.0003
    vf_lr: 0.0003
    policy_mean_reg_weight: 0.001
    policy_std_reg_weight: 0.001
    target_entropy: -3.0

  env_specs:
    env_name: 'walker'
    env_kwargs: {}
